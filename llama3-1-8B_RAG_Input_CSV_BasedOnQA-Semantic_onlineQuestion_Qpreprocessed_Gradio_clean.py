#------------------------------------------------------------------------------
# Step 0: Import required libraries and initialize constants and thresholds
#------------------------------------------------------------------------------
import time
from typing import Optional, List
from langchain.schema import Document
import glob
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
import gradio as gr
from langchain.embeddings import HuggingFaceEmbeddings
import unicodedata
from langchain_ollama import ChatOllama
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Thresholds and model initialization
qa_similarity_threshold = 0.87 # Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„ (have been tested)
retriever_similarity_threshold = 0.85 # Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø¯ÙˆÙ… (have been tested)
init_temperature = 0.3 # Ù…Ù‚Ø§Ø¯ÛŒØ² Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡â€ŒØªØ± (Ø¨Ø§Ø²Ù‡ 0-2)
#embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")
embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
client_embedding_model = embedding_model.client  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ù…Ø§Ù† Ù…Ø¯Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± HuggingFaceEmbeddings

#------------------------------------------------------------------------------
# Step 1: Preprocess questions (remove punctuation and normalize whitespace)
#------------------------------------------------------------------------------
def preprocess_question(text: str) -> str:
    text = re.sub(r'[\d\.\ØŒ\;\:\?\!\"\(\)\[\]Â«Â»]', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

#------------------------------------------------------------------------------
# Step 2: Load and embed FAQ questions
#------------------------------------------------------------------------------
print("Loading FAQ data...")
str1 = "articles/QA_files/FAQ_Original_new29.csv"
print(str1)
qa_df = pd.read_csv(str1)
questionFAQ_embeddings = client_embedding_model.encode(qa_df['question'].tolist())

#------------------------------------------------------------------------------
# Step 3: Load and convert CSV chunks to Document objects
#------------------------------------------------------------------------------
csv_files = glob.glob("articles/csv_files/chunks_Original_new29.csv")
doc_splits = []

for csv_file in csv_files:
    df = pd.read_csv(csv_file)
    for _, row in df.iterrows():
        doc_splits.append(Document(
            page_content=row['page_content'],
            metadata={'source': row['source']}
        ))

#------------------------------------------------------------------------------
# Step 4: Custom retriever with similarity threshold
#------------------------------------------------------------------------------
class ThresholdRetriever:
    def __init__(self, documents,client_embedding_model):
        self.documents = documents
        self.client_embedding_model = client_embedding_model
        self.doc_embeddings = client_embedding_model.encode([doc.page_content for doc in documents])

    def invoke(self, query: str, k: int = 5) -> List[tuple[Document, float]]:
        query_embedding = self.client_embedding_model.encode([query])
        similarities = cosine_similarity(query_embedding, self.doc_embeddings)[0]
        top_indices = np.argsort(similarities)[-k:][::-1]
        results = [(self.documents[i], similarities[i]) for i in top_indices if similarities[i] >= retriever_similarity_threshold]
        print(f"Top similarities: {similarities[top_indices]}")
        return results

retriever = ThresholdRetriever(documents=doc_splits,client_embedding_model=client_embedding_model)

#------------------------------------------------------------------------------
# Step 5: Prompt setup for each layer of RAG
#------------------------------------------------------------------------------
qa_prompt = PromptTemplate(
    template="""Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø²ÛŒØ± Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ø¨Ø¯Ù‡. ÙÙ‚Ø· Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
        1. Ù¾Ø§Ø³Ø® Ø±Ø§ Ø±ÙˆØ§Ù†ØŒ Ø·Ø¨ÛŒØ¹ÛŒØŒ Ø±Ø³Ù…ÛŒ Ùˆ Ú©Ø§Ù…Ù„Ø§ ÙØ§Ø±Ø³ÛŒ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯

    Ù…ØªÙ† Ù…Ø±Ø¬Ø¹:
    {reference_text}
    
    Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±:
    {question}
    
Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø±Ø³Ù…ÛŒ:""",
    input_variables=["reference_text", "question"],
)

search_prompt = PromptTemplate(
    template="""Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø²ÛŒØ± Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ø¨Ø¯Ù‡. ÙÙ‚Ø· Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
        1. Ù¾Ø§Ø³Ø® Ø±Ø§ Ø±ÙˆØ§Ù†ØŒ Ø·Ø¨ÛŒØ¹ÛŒØŒ Ø±Ø³Ù…ÛŒ Ùˆ Ú©Ø§Ù…Ù„Ø§ ÙØ§Ø±Ø³ÛŒ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯

    **Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±**: 
    {question}
    
    **Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±ØªØ¨Ø·**:
    {documents}
Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø±Ø³Ù…ÛŒ:""",
    input_variables=["question", "documents"],
)

general_prompt = PromptTemplate(
    template="""
Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ú¯ÙØªâ€Œ Ùˆ Ú¯ÙˆÚ¯Ø± ÙØ§Ø±Ø³ÛŒâ€Œ Ø²Ø¨Ø§Ù†ØŒ Ù…ÙˆØ¯Ø¨ØŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ùˆ **Ø¨Ø§ Ø´Ø®ØµÛŒØª Ø§Ù†Ø³Ø§Ù†ÛŒ (Ù…ÙØ±Ø¯)** Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒØŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ùˆ Ø¯Ø§Ù†Ø´ÛŒ Ø±Ø§ Ø¯Ø§Ø±ÛŒØ¯.

Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„â€ŒÙ‡Ø§:

1. Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ ÙÙ‚Ø· Ø¨Ø§ Ø¶Ù…ÛŒØ± Â«Ù…Ù†Â» ØµØ­Ø¨Øª Ú©Ù†ÛŒØ¯ Ùˆ Ø§Ø² Ø§ÙØ¹Ø§Ù„ Ø§ÙˆÙ„â€Œ Ø´Ø®Øµ Ù…ÙØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯. Ø§Ø² Ø§ÙØ¹Ø§Ù„ Ø¬Ù…Ø¹ (Ù…Ø«Ù„ "Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…"ØŒ "Ù‡Ø³ØªÛŒÙ…") Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯.
2. Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ø­Ø§Ù„Øª Ø§Ø­ÙˆØ§Ù„â€ŒÙ¾Ø±Ø³ÛŒ ÛŒØ§ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ø¯Ø§Ø±Ø¯ (Ù…Ø«Ù„Ø§Ù‹ Â«Ø³Ù„Ø§Ù…Â»ØŒ Â«Ø®ÙˆØ¨ÛŒØŸÂ»ØŒ Â«Ú†Ù‡ Ø®Ø¨Ø±ØŸÂ»ØŒ Â«ØªÙˆ Ú©ÛŒ Ù‡Ø³ØªÛŒØŸÂ») Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯:
   - Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù…Ø¹ÛŒØ§Ø±ØŒ Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡ØŒ Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø¨Ø§Ø´Ø¯.
   - Ø§Ø² ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ (Ù…Ø«Ù„ Â«Ù…Ø±Ø­Ø¨Ø§Â») Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯.
   - Ø¬Ù…Ù„Ù‡â€Œ Ø¨Ù†Ø¯ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ø¶Ù…ÛŒØ± Ùˆ ÙØ¹Ù„ Ù‡Ù…â€Œ Ø±Ø§Ø³ØªØ§ Ø¨Ø§Ø´Ù†Ø¯.
   - Ø§Ø² Ù¾Ø±Ø³ÛŒØ¯Ù† Ø³Ø¤Ø§Ù„ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ (Ù…Ø«Ù„ Â«Ú†ÛŒ Ø®Ø¨Ø±ØŸÂ») Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ø´ÙˆØ¯.
3. Ø§Ú¯Ø± Ø³ÙˆØ§Ù„Ø§Øª Ù¾Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø§Ù‡ Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª (ÛŒØ§ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø§Ù‡ Ù…Ø®Ø§Ø¨Ø±Ø§Øª) Ù†Ø¨ÙˆØ¯ØŒ  Ù¾Ø§Ø³Ø®: Ù„Ø·ÙØ§ Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø§Ù‡ Ù…Ø®Ø§Ø¨Ø±Ø§Øª Ø¨Ù¾Ø±Ø³ÛŒØ¯ ØªØ§ Ø¨ØªÙˆØ§Ù†Ù… Ø¨Ù‡ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù…
4. Ù¾Ø§Ø³Ø® ÙÙ‚Ø· Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§Ø´Ø¯ Ùˆ Ø§Ø² Ù‡ÛŒÚ† Ø²Ø¨Ø§Ù† Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´ÙˆØ¯.

Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§:
- Ø³ÙˆØ§Ù„: Ø³Ù„Ø§Ù… â†’ Ù¾Ø§Ø³Ø®: Ø³Ù„Ø§Ù…! ÙˆÙ‚Øª Ø¨Ø®ÛŒØ±ØŒ Ú†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ú©Ù†Ù…ØŸ
- Ø³ÙˆØ§Ù„: Ø®ÙˆØ¨ÛŒØŸ â†’ Ù¾Ø§Ø³Ø®: Ù…Ù…Ù†ÙˆÙ†Ù…ØŒÙ„Ø·ÙØ§ Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø§Ù‡ Ù…Ø®Ø§Ø¨Ø±Ø§Øª Ø¨Ù¾Ø±Ø³ÛŒØ¯ ØªØ§ Ø¨ØªÙˆØ§Ù†Ù… Ø¨Ù‡ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù….
- Ø³ÙˆØ§Ù„: Ú†Ø·ÙˆØ±ÛŒØŸ ÛŒØ§ Ú†Ø·ÙˆØ±ÛŒ â†’ Ù¾Ø§Ø³Ø®: Ù…Ù…Ù†ÙˆÙ†Ù…ØŒ Ù„Ø·ÙØ§ Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø§Ù‡ Ù…Ø®Ø§Ø¨Ø±Ø§Øª Ø¨Ù¾Ø±Ø³ÛŒØ¯ ØªØ§ Ø¨ØªÙˆØ§Ù†Ù… Ø¨Ù‡ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù….

Ø§Ú©Ù†ÙˆÙ† Ù„Ø·ÙØ§Ù‹ Ø¨Ù‡ Ù¾Ø±Ø³Ø´ Ø²ÛŒØ± Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯:

Ø³ÙˆØ§Ù„:
{question}

Ù¾Ø§Ø³Ø®:""",
    input_variables=["question"],
)


llm = ChatOllama(model="llama3.1", temperature=init_temperature)
qa_chain = qa_prompt | llm | StrOutputParser()
search_chain = search_prompt | llm | StrOutputParser()
general_chain = general_prompt | llm | StrOutputParser()

#------------------------------------------------------------------------------
# Step 6: Main RAG application with three fallback layers
#------------------------------------------------------------------------------
class IntelligentRAGApplication:
    def __init__(self, retriever, qa_chain, search_chain, general_chain, qa_df, questionFAQ_embeddings, client_embedding_model):
        self.retriever = retriever
        self.qa_chain = qa_chain
        self.search_chain = search_chain
        self.general_chain = general_chain
        self.qa_df = qa_df
        self.questionFAQ_embeddings = questionFAQ_embeddings
        self.client_embedding_model = client_embedding_model


    def find_most_similar_question(self, processed_question: str) -> tuple[Optional[str], float]:
        question_embedding = self.client_embedding_model.encode([processed_question])
        similarities = cosine_similarity(question_embedding, self.questionFAQ_embeddings)
        max_index = np.argmax(similarities)
        max_similarity = similarities[0][max_index]
        if max_similarity >= qa_similarity_threshold:
            return self.qa_df.iloc[max_index]['original_chunk'], max_similarity
        return None, max_similarity

    def run(self, question: str) -> tuple[str, str]:
        source = None
        processed_question = preprocess_question(question)

        # Layer 1: FAQ-based matching
        reference_text, similarity_score = self.find_most_similar_question(processed_question)
        print(f"QA similarity_score: {similarity_score}")
        if reference_text:
            source = self.qa_df.iloc[np.argmax(
                cosine_similarity(
                    self.client_embedding_model.encode([processed_question]),
                    self.questionFAQ_embeddings
                )[0]
            )]['source']
            print("[Layer 1 - FAQ match]")
            answer = self.qa_chain.invoke({"reference_text": reference_text, "question": processed_question})
            return answer, source

        # Layer 2: Semantic retrieval
        docs_and_scores = self.retriever.invoke(processed_question)
        if docs_and_scores:
            top_docs = docs_and_scores[:5]# ÙÙ‚Ø· 5 Ø³Ù†Ø¯ Ø¨Ø±ØªØ±
            best_score = top_docs[0][1]
            if best_score >= retriever_similarity_threshold:
                sources = list({doc.metadata['source'] for doc, _ in top_docs})
                source = " | ".join(sources)
                print("[Layer 2 - Semantic search]")
                doc_texts = "\n\n---\n\n".join([doc.page_content for doc, _ in top_docs])
                answer = self.search_chain.invoke({"question": processed_question, "documents": doc_texts})
                return answer, source

        # Layer 3: General knowledge fallback
        print("[Layer 3 - General knowledge fallback]")
        return self.general_chain.invoke({"question": processed_question}), "Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ Ù…Ø¯Ù„"

rag_application = IntelligentRAGApplication(
    retriever=retriever,
    qa_chain=qa_chain,
    search_chain=search_chain,
    general_chain=general_chain,
    qa_df=qa_df,
    questionFAQ_embeddings=questionFAQ_embeddings,
    client_embedding_model=client_embedding_model
    
)

#------------------------------------------------------------------------------
# Step 7: Gradio interface
#------------------------------------------------------------------------------
def clean_output(text: str) -> str:
    try:
        return unicodedata.normalize("NFKC", text).encode("utf-8", errors="ignore").decode("utf-8")
    except Exception:
        return text

def chat_fn(message, history):
    if not message.strip():
        return "Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ù…Ø¹ØªØ¨Ø±ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯."
    try:
        start_time = time.time()
        answer, source = rag_application.run(message)
        answer = clean_output(answer)
        elapsed_time = time.time() - start_time
        return f"Ù¾Ø§Ø³Ø®:\n{answer}\n\nÙ…Ù†Ø¨Ø¹: {source}\nâ±ï¸ Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ: {elapsed_time:.2f} Ø«Ø§Ù†ÛŒÙ‡"
    except Exception as e:
        return f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„: {str(e)}"

with gr.Blocks(css="""
#custom-textbox textarea {
    background-color: #f3f0ff !important;
    border: 1px solid #ccc !important;
    direction: rtl;
    text-align: right;
}
.footer {
    text-align: center;
    color: gray;
    margin-top: 1rem;
    direction: rtl;
}
""") as demo:
    gr.ChatInterface(
        fn=chat_fn,
        title="ğŸ¤– Ú†Øªâ€ŒØ¨Ø§Øª Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø§Ù‡ Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ğŸ¤–",
        chatbot=gr.Chatbot(label="Ú¯ÙØªÚ¯Ùˆ Ø¨Ø§ Ú†Øªâ€ŒØ¨Ø§Øª", rtl=True),
        textbox=gr.Textbox(
            placeholder="Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯...",
            rtl=True,
            container=True,
            elem_id="custom-textbox"
        ),
        description="Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ØŒ Ø¹Ø¨Ø§Ø±Øª 'Ø®Ø±ÙˆØ¬' ÛŒØ§ 'Ù¾Ø§ÛŒØ§Ù†' Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯."
    )
    gr.HTML("<div class='footer'>Developed by Toktam Zoughi</div>")

demo.launch()
